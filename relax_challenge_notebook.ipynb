{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relax Challenge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import 'takehome_users.csv' as dataframe 'user'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = pd.read_csv(\"takehome_users.csv\", parse_dates=['creation_time'], encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "user['last_session_creation_time'] = pd.to_datetime(user.last_session_creation_time, unit='s') # convert unix timestamp to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>object_id</th>\n",
       "      <th>creation_time</th>\n",
       "      <th>name</th>\n",
       "      <th>email</th>\n",
       "      <th>creation_source</th>\n",
       "      <th>last_session_creation_time</th>\n",
       "      <th>opted_in_to_mailing_list</th>\n",
       "      <th>enabled_for_marketing_drip</th>\n",
       "      <th>org_id</th>\n",
       "      <th>invited_by_user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2014-04-22 03:53:30</td>\n",
       "      <td>Clausen August</td>\n",
       "      <td>AugustCClausen@yahoo.com</td>\n",
       "      <td>GUEST_INVITE</td>\n",
       "      <td>2014-04-22 03:53:30</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>10803.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2013-11-15 03:45:04</td>\n",
       "      <td>Poole Matthew</td>\n",
       "      <td>MatthewPoole@gustr.com</td>\n",
       "      <td>ORG_INVITE</td>\n",
       "      <td>2014-03-31 03:45:04</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>316.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2013-03-19 23:14:52</td>\n",
       "      <td>Bottrill Mitchell</td>\n",
       "      <td>MitchellBottrill@gustr.com</td>\n",
       "      <td>ORG_INVITE</td>\n",
       "      <td>2013-03-19 23:14:52</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>94</td>\n",
       "      <td>1525.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2013-05-21 08:09:28</td>\n",
       "      <td>Clausen Nicklas</td>\n",
       "      <td>NicklasSClausen@yahoo.com</td>\n",
       "      <td>GUEST_INVITE</td>\n",
       "      <td>2013-05-22 08:09:28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5151.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2013-01-17 10:14:20</td>\n",
       "      <td>Raw Grace</td>\n",
       "      <td>GraceRaw@yahoo.com</td>\n",
       "      <td>GUEST_INVITE</td>\n",
       "      <td>2013-01-22 10:14:20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>193</td>\n",
       "      <td>5240.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   object_id       creation_time               name  \\\n",
       "0          1 2014-04-22 03:53:30     Clausen August   \n",
       "1          2 2013-11-15 03:45:04      Poole Matthew   \n",
       "2          3 2013-03-19 23:14:52  Bottrill Mitchell   \n",
       "3          4 2013-05-21 08:09:28    Clausen Nicklas   \n",
       "4          5 2013-01-17 10:14:20          Raw Grace   \n",
       "\n",
       "                        email creation_source last_session_creation_time  \\\n",
       "0    AugustCClausen@yahoo.com    GUEST_INVITE        2014-04-22 03:53:30   \n",
       "1      MatthewPoole@gustr.com      ORG_INVITE        2014-03-31 03:45:04   \n",
       "2  MitchellBottrill@gustr.com      ORG_INVITE        2013-03-19 23:14:52   \n",
       "3   NicklasSClausen@yahoo.com    GUEST_INVITE        2013-05-22 08:09:28   \n",
       "4          GraceRaw@yahoo.com    GUEST_INVITE        2013-01-22 10:14:20   \n",
       "\n",
       "   opted_in_to_mailing_list  enabled_for_marketing_drip  org_id  \\\n",
       "0                         1                           0      11   \n",
       "1                         0                           0       1   \n",
       "2                         0                           0      94   \n",
       "3                         0                           0       1   \n",
       "4                         0                           0     193   \n",
       "\n",
       "   invited_by_user_id  \n",
       "0             10803.0  \n",
       "1               316.0  \n",
       "2              1525.0  \n",
       "3              5151.0  \n",
       "4              5240.0  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12000 entries, 0 to 11999\n",
      "Data columns (total 10 columns):\n",
      " #   Column                      Non-Null Count  Dtype         \n",
      "---  ------                      --------------  -----         \n",
      " 0   object_id                   12000 non-null  int64         \n",
      " 1   creation_time               12000 non-null  datetime64[ns]\n",
      " 2   name                        12000 non-null  object        \n",
      " 3   email                       12000 non-null  object        \n",
      " 4   creation_source             12000 non-null  object        \n",
      " 5   last_session_creation_time  8823 non-null   datetime64[ns]\n",
      " 6   opted_in_to_mailing_list    12000 non-null  int64         \n",
      " 7   enabled_for_marketing_drip  12000 non-null  int64         \n",
      " 8   org_id                      12000 non-null  int64         \n",
      " 9   invited_by_user_id          6417 non-null   float64       \n",
      "dtypes: datetime64[ns](2), float64(1), int64(4), object(3)\n",
      "memory usage: 937.6+ KB\n"
     ]
    }
   ],
   "source": [
    "user.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing data are present in the 'last_session_creation_time' column and the 'invited_by_user_id' column. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import \"takehome_user_engagement.csv\" as dataframe 'engagement'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "engagement = pd.read_csv(\"takehome_user_engagement.csv\", parse_dates=['time_stamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_stamp</th>\n",
       "      <th>user_id</th>\n",
       "      <th>visited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-04-22 03:53:30</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-11-15 03:45:04</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-11-29 03:45:04</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-12-09 03:45:04</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-12-25 03:45:04</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           time_stamp  user_id  visited\n",
       "0 2014-04-22 03:53:30        1        1\n",
       "1 2013-11-15 03:45:04        2        1\n",
       "2 2013-11-29 03:45:04        2        1\n",
       "3 2013-12-09 03:45:04        2        1\n",
       "4 2013-12-25 03:45:04        2        1"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engagement.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 207917 entries, 0 to 207916\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype         \n",
      "---  ------      --------------   -----         \n",
      " 0   time_stamp  207917 non-null  datetime64[ns]\n",
      " 1   user_id     207917 non-null  int64         \n",
      " 2   visited     207917 non-null  int64         \n",
      "dtypes: datetime64[ns](1), int64(2)\n",
      "memory usage: 4.8 MB\n"
     ]
    }
   ],
   "source": [
    "engagement.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown, the 'engagement' dataframe is the usage summary table that keeps a log as an entry/row for each day that a user logged into the product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. data wrangling\n",
    "\n",
    "To tackle the problem of predicting 'adopted user', the adopted users need to be identified first. 'adopted user' is defined as a user who has logged into the product on three separate days in at least one seven-day period, therefore the 'engagement' dataframe should be used to compute whether a user is an 'adopted user' or not. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare dataframes 'user' and 'engagement' for computing 'adopted user'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "user.set_index('object_id',inplace=True) # set 'object_id' as the index of 'user'\n",
    "user.index.name = 'user_id' # update the name of the index to be consistent with other dataframe 'engagement'\n",
    "user['adopted_user'] = 0 # initiate 'adopted_user' column to 0 (non-adopted user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that there is no duplicated entry for the same user on the same day in the 'engagement' dataframe\n",
    "copy = engagement.copy()\n",
    "copy['time_stamp'] = copy.time_stamp.dt.date\n",
    "assert (copy.groupby(['user_id','time_stamp']).visited.count() == 1).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate 'adopted user'\n",
    "\n",
    "# first sort the dataframe by 'user_id' first, then 'time_stamp'\n",
    "engagement = engagement.sort_values(['user_id','time_stamp']) \n",
    "\n",
    "# loop over the 'engagement' dataframe, and check if 'user_id' is the same as the following two entries,\n",
    "# if so, check if their timestamps are within a seven-day period\n",
    "for idx in engagement.index[:-2]:\n",
    "    if engagement.loc[idx, 'user_id'] == engagement.loc[idx+2, 'user_id']:\n",
    "        if (engagement.loc[idx+2,'time_stamp'] - engagement.loc[idx,'time_stamp']).days <= 7:\n",
    "            user_id = engagement.loc[idx,'user_id']\n",
    "            user.loc[user_id, 'adopted_user'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user.adopted_user.value_counts() # count adopted users vs non-adopted users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown, there are 1656 adopted users, 10344 non-adopted users out of a total of 12000 users. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the missing data in the 'last_session_creation_time' column\n",
    "user[user.last_session_creation_time.isnull()].adopted_user.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown, all entries with missing data in the 'last_session_creation_time' and 'time_delta' columns correspond to 'non-adopted' users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing data for the 'invited_by_user_id' column by transforming the column\n",
    "user['invited'] = user.invited_by_user_id.apply(lambda x: 0 if np.isnan(x) else 1)\n",
    "user.invited.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.countplot(x=\"adopted_user\", data=user)\n",
    "ax.set_title('Distribution of users based on their adoption of the product');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown, the majority of the users are not adopted users (86.2%). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(user.corr(), cmap='coolwarm', annot=True);\n",
    "user.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the correlation matrix for a quick insight on the potential correlations between numeric columns. \n",
    "As shown, there is a weak correlation between user adoption ('adpoted_user') and their organization ('org_id'). \n",
    "In addition, 'opted_in_to_mailing_list' and 'enabled_for_marketing_drip' columns are related. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at categorical and datetime columns\n",
    "plt.figure(figsize=(20,10));\n",
    "ax1 = plt.subplot(2,2,1)\n",
    "ax1 = sns.countplot(x='creation_source', hue='adopted_user', data=user, ax=ax1)\n",
    "ax1.set_title('User adoption by user creation source');\n",
    "\n",
    "ax2 = plt.subplot(2,2,2)\n",
    "ax2 = sns.countplot(x=user.invited, hue=user.adopted_user, ax=ax2)\n",
    "ax2.set_title('User adoption by whether they are invited or not');\n",
    "\n",
    "ax3 = plt.subplot(2,2,3)\n",
    "ax3 = sns.countplot(x=user.creation_time.dt.year, hue=user.adopted_user, ax=ax3)\n",
    "ax3.set_title('User adoption by the user account creation year');\n",
    "\n",
    "ax4 = plt.subplot(2,2,4)\n",
    "ax4 = sns.countplot(x=user.last_session_creation_time.dt.year, hue=user.adopted_user, ax=ax4)\n",
    "ax4.set_title('User adoption by the year of their last session');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown, there is a weak dependence between user adoption and user creation source, when users signed up via 'GUEST_INVITE' and 'ORG_INVITE' have higher ratio of adoption, whereas users signed up via 'PERSONAL_PROJECT' has the lowest ratio of adoption.\n",
    "\n",
    "A strong correlation between user adoption and their activity timestamp is observed. In particular, users signed up earlier has higher adoption rate, and users with more recent activities on the product ('last_session_creation_time') have higher adoption rate. This suggests that the time difference between a user's account creation time and the user's last session time might be an importance indicator as well, where a user who signed up early and still has very recent activity is likely to be an adopted user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at 'engagement' dataframe\n",
    "print(engagement.groupby('user_id').visited.count().quantile([0.25, 0.75, 0.8, 0.85, 0.9, 0.95]))\n",
    "\n",
    "ax = sns.distplot(engagement.groupby('user_id').visited.count(),kde=False)\n",
    "ax.set_yscale('log');\n",
    "ax.set_title('Distribution of users based on their total number of visits to the product');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown, most of the users (> 75%) have visited the product no more than once, and these users are likely to be non-adpoted users. A feature out of users' visit activity can be very valuable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Feature engineering\n",
    "Based on the insights from the above EDA, two new features are introduced: \n",
    "1. 'time_delta': represents the time difference between a user's last session and their account creation date\n",
    "2. 'total_visits': represents the total number of visits the user has to the product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a new feature of 'time_delta', representing the time difference between a user's last session and their account creation date\n",
    "\n",
    "user['time_delta'] = user.last_session_creation_time - user.creation_time\n",
    "# fill in the missing data in the newly created 'time_delta' column (resulting from the missing data in the 'last_session_creation_time' column)\n",
    "# fillna using the minimum value of the column, since all the missing data correspond to non-adopted user\n",
    "user['time_delta'] = user.time_delta.fillna(user.time_delta.min())\n",
    "\n",
    "ax = sns.countplot(x=user.time_delta.dt.days//100, hue=user.adopted_user)\n",
    "ax.set_title('User adoption by the year of their last session');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown, the 'time_delta' strongly correlates to whether a user is an adopted user or not. Users with a time delta of less than 100 days are very likely to be non-adopted users, whereas users with a time delta of more than 200 days are all adopted users. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a new feature of 'total_visits', representing the total number of visits the user has to the product\n",
    "\n",
    "total_visits = engagement.groupby('user_id').visited.count()\n",
    "total_visits.name = 'total_visits'\n",
    "\n",
    "user2 = user.join(total_visits, how='left')\n",
    "# fill in missing data with 0 since no entry in 'engagement' dataframe means no visit\n",
    "user2['total_visits'] = user2.total_visits.fillna(0).astype('int') \n",
    "\n",
    "# understand correlation between 'total_visits' and 'adopted_user'\n",
    "print(user2.total_visits.quantile([0.75, 0.8, 0.825, 0.85, 0.875, 0.9, 0.95]))\n",
    "print(user2[user2.total_visits < 3].adopted_user.value_counts())\n",
    "print(user2[user2.total_visits > 21].adopted_user.value_counts())\n",
    "\n",
    "print(user2[user2.adopted_user == 1].total_visits.min(), user2[user2.adopted_user == 1].total_visits.max())\n",
    "print(user2[user2.adopted_user == 0].total_visits.min(), user2[user2.adopted_user == 0].total_visits.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown, the total number of visits strongly correlates to whether a user is an adopted user or not, therefore an important indicator of user adoption.<br> \n",
    "\n",
    "\n",
    "In addition, these two new features 'time_delta' and 'total_visits' will be important features to the machine learning models for predicting user adoption. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Machine learning modeling\n",
    "\n",
    "To predict user adoption of the product, three different classification algorithms are experimented: \n",
    "1. linear Logistic Regression classification algorithm\n",
    "2. non-linear Support Vector Machine classification algorithm\n",
    "3. ensembles of tree-based Random Forest classification algorithm \n",
    "\n",
    "Due to the fact there are only 1656 adopted users out of 12000 users (1656 positive vs. 10344 negative), this is considered as an imbalanced classification problem, f1_score and area_under_ROC_curve (roc_auc_score) are selected instead of the accuracy score for evaluating the performance of the classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dataframe for modeling\n",
    "\n",
    "# convert to cateogorical type\n",
    "user2['adopted_user'] = user2.adopted_user.astype('category')\n",
    "user2['opted_in_to_mailing_list'] = user2.opted_in_to_mailing_list.astype('category')\n",
    "user2['enabled_for_marketing_drip'] = user2.enabled_for_marketing_drip.astype('category')\n",
    "user2['invited'] = user2.invited.astype('category')\n",
    "\n",
    "# user datetime.toordinal() to convert the 'creation_time' column (later is rescaled using sklearn standardscaler)\n",
    "from datetime import datetime\n",
    "user2['creation_time_ordinal'] = user2.creation_time.apply(datetime.toordinal)\n",
    "\n",
    "# user timedelta.dt.days to convert 'time_delta' columns\n",
    "user2['time_delta_day'] = user2.time_delta.dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features and target\n",
    "X = user2.drop(['creation_time','name','email','last_session_creation_time','invited_by_user_id','adopted_user',\\\n",
    "              'time_delta'], axis=1)\n",
    "y = user2.adopted_user\n",
    "\n",
    "# convert categorical columns to dummy variables\n",
    "X = pd.get_dummies(X, drop_first=True)  # convert categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate the score table\n",
    "index = ['LogisticRegression','SVC','RandomForestClassifier']\n",
    "score_table = pd.DataFrame(index = index, columns= ['f1_score_train','auc_train','f1_score_test','auc_test'])\n",
    "\n",
    "# define function for plotting the results\n",
    "def compute_log_result(algo, pred_train, pred_test):\n",
    "    \"\"\"compute and log the performance for both training and test sets\"\"\"\n",
    "    \n",
    "    # compute the performance\n",
    "    # since this classification problem is an imbalanced case (1656 positive vs. 10344 negative),\n",
    "    # f1_score and area_under_ROC_curve (roc_auc_score) are used instead of accuracy for evaluating the performance of the models\n",
    "    \n",
    "    f1_train = f1_score(y_train, pred_train)\n",
    "    f1_test = f1_score(y_test, pred_test)\n",
    "    auc_train = roc_auc_score(y_train, pred_train)\n",
    "    auc_test = roc_auc_score(y_test, pred_test)\n",
    "    \n",
    "    # log the performance\n",
    "    score_table.loc[algo,:] = f1_train, auc_train, f1_test, auc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit logistic regression model with default parameters\n",
    "\n",
    "logit = Pipeline([('scaler', StandardScaler()),('logit',LogisticRegression())])\n",
    "logit.fit(X_train, y_train)\n",
    "pred_train = logit.predict(X_train)\n",
    "pred_test = logit.predict(X_test)\n",
    "\n",
    "# print features and their coefficients\n",
    "feature_coef = pd.DataFrame({'feature':X_train.columns, 'coefficient':logit.named_steps.logit.coef_[0]})\n",
    "print(feature_coef.sort_values('coefficient',ascending=False))\n",
    "\n",
    "# logging of model performance\n",
    "compute_log_result(\"LogisticRegression\", pred_train, pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit support vector machine model with default parameters\n",
    "\n",
    "svc = Pipeline([('scaler', StandardScaler()),('svc', SVC())])\n",
    "svc.fit(X_train, y_train)\n",
    "pred_train = svc.predict(X_train)\n",
    "pred_test = svc.predict(X_test)\n",
    "\n",
    "# logging of model performance\n",
    "compute_log_result(\"SVC\", pred_train, pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fit random forest model with default parameters\n",
    "\n",
    "rfc = Pipeline([('scaler', StandardScaler()),('rfc', RandomForestClassifier(class_weight='balanced'))]) # set class_weight='balanced' to adjust for the imbalanced classes\n",
    "rfc.fit(X_train, y_train)\n",
    "pred_train = rfc.predict(X_train)\n",
    "pred_test = rfc.predict(X_test)\n",
    "\n",
    "# print feature importance\n",
    "feature_rank = pd.DataFrame({'feature': X_train.columns, 'importance': rfc.named_steps.rfc.feature_importances_})\n",
    "print(feature_rank.sort_values(by='importance',ascending=False))\n",
    "\n",
    "compute_log_result(\"RandomForestClassifier\", pred_train, pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_table # print model performance with default parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization of RandomForestClassifier\n",
    "As shown, with the default parameters, RandomForestClassifier gives the best performance among the three, it also allows to adjust for the imbalanced classes by setting 'class_weight' parameter. Therefore, parameter tuning of the RandomForestClassifier model is further performed using gridsearch cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search optimization of RandomForestClassifier\n",
    "# Parameter tuning of rfc is desired, as the result from default parameters suggest overfitting to the training set\n",
    "\n",
    "# Grid search of parameters n_estimators and max_features to improve RandomForestRegressor model\n",
    "parameters = {'rfc__n_estimators': np.arange(20,90,5), 'rfc__max_features':['auto','sqrt','log2']}\n",
    "# f1_score is used as the selection criteria since f1_score gives better sensitivity (wider range of score distribution) with imbalanced case\n",
    "rfc_cv = GridSearchCV(rfc, parameters, scoring='f1', cv=5)\n",
    "rfc_cv.fit(X_train, y_train)\n",
    "print('Best parameters:', rfc_cv.best_params_)\n",
    "print('Corresponding f1 score:', rfc_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updated RandomForestClassification model with the optimized parameter\n",
    "rfc = Pipeline([('scaler', StandardScaler()),('rfc', RandomForestClassifier(n_estimators=55, max_features='log2', class_weight='balanced'))])\n",
    "rfc.fit(X_train, y_train)\n",
    "pred_train = rfc.predict(X_train)\n",
    "pred_test = rfc.predict(X_test)\n",
    "\n",
    "compute_log_result(\"RandomForestClassifier\", pred_train, pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final results\n",
    "print(score_table.loc['RandomForestClassifier'])\n",
    "\n",
    "# feature importance ranking from the optimized RandomForestClassifier \n",
    "feature_rank = pd.DataFrame({'feature': X_train.columns, 'importance': rfc.named_steps.rfc.feature_importances_})\n",
    "print(feature_rank.sort_values(by='importance',ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown, the optimized RandomForestClassifier gives a pretty good f1 score and auc score. The feature ranking from the optimized classifier suggests that users' total number of visits to the product ('total_visits') and the time difference between a user's account creation and his last activity are the two critical factors indicating a user's adoption of the product, in good agreement with the EDA and feature engineering findings. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
